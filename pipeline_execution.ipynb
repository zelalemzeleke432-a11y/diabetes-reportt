{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31184fdc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# pipeline_execution.ipynb\n",
    "# This notebook executes the entire data preparation pipeline by calling functions\n",
    "# from the 'data_preprocessing_functions' module.\n",
    "\n",
    "# --- 1. SETUP AND IMPORTS ---\n",
    "\n",
    "# Import standard libraries for analysis and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from collections import Counter\n",
    "\n",
    "# Import all custom functions and constants from the local Python module\n",
    "from data_preprocessing_functions import (\n",
    "    load_data,\n",
    "    replace_impossible_zeros,\n",
    "    impute_missing_values,\n",
    "    treat_outliers_iqr,\n",
    "    feature_engineering,\n",
    "    encode_features,\n",
    "    scale_data,\n",
    "    feature_selection_kbest,\n",
    "    perform_pca,\n",
    "    handle_imbalance_smote,\n",
    "    FEATURES_WITH_IMPOSSIBLE_ZEROS\n",
    ")\n",
    "\n",
    "# --- 2. CONFIGURATION AND CONSTANTS ---\n",
    "\n",
    "# Input/Output File Paths\n",
    "INPUT_FILE = 'Diabetes Missing Data.csv'\n",
    "OUTPUT_P1 = 'diabetes_initial_clean.csv'\n",
    "OUTPUT_P2 = 'diabetes_cleaned.csv'\n",
    "OUTPUT_P3 = 'diabetes_transformed_scaled.csv'\n",
    "OUTPUT_P4 = 'diabetes_reduced.csv'\n",
    "FINAL_OUTPUT_FILE = 'diabetes_final_balanced.csv'\n",
    "\n",
    "# Define columns for Outlier Treatment in Phase 2 (Typically all continuous features + Pregnant)\n",
    "OUTLIER_COLS = ['Pregnant', 'Glucose', 'Diastolic_BP', 'Skin_Fold', 'Serum_Insulin', 'BMI', 'Diabetes_Pedigree', 'Age']\n",
    "\n",
    "# Define Labels for Age_Group Encoding in Phase 3\n",
    "AGE_GROUP_LABELS = ['Young', 'Middle-Aged', 'Senior', 'Elderly']\n",
    "K_BEST_FEATURES = 5 # Number of features to select in Phase 4\n",
    "\n",
    "print(\"Pipeline Configuration Loaded.\")\n",
    "print(f\"Features where Zero is replaced by NaN: {FEATURES_WITH_IMPOSSIBLE_ZEROS}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. PHASE 1: DATA LOADING AND ZERO REPLACEMENT ---\n",
    "print(\"--- PHASE 1: Data Loading & Initial Cleaning ---\")\n",
    "\n",
    "# Task 1: Load Data\n",
    "df = load_data(INPUT_FILE)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"FATAL ERROR: Could not load the dataset. Check file path.\")\n",
    "else:\n",
    "    print(f\"1. Dataset loaded successfully. Shape: {df.shape}\")\n",
    "\n",
    "    # Task 2: Replace impossible zeros (biologically impossible values) with NaN\n",
    "    df_p1 = replace_impossible_zeros(df, FEATURES_WITH_IMPOSSIBLE_ZEROS)\n",
    "    print(\"2. Impossible zeros replaced with NaN.\")\n",
    "    print(\"   NaN counts after replacement:\")\n",
    "    print(df_p1.isnull().sum())\n",
    "\n",
    "    # Task 3: Visualize Missing Data (Jupyter-specific visualization)\n",
    "    print(\"\\n3. Missing Data Visualization (Matrix):\")\n",
    "    msno.matrix(df_p1, figsize=(10, 5), fontsize=10)\n",
    "    plt.title(\"Missing Data Matrix (Post Zero-Replacement)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Save the processed DataFrame for the next phase\n",
    "    df_p1.to_csv(OUTPUT_P1, index=False)\n",
    "    print(f\"4. DataFrame state saved to: {OUTPUT_P1}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 4. PHASE 2: DATA CLEANING (IMPUTATION AND OUTLIER TREATMENT) ---\n",
    "print(\"--- PHASE 2: Data Cleaning (Imputation & Outlier Treatment) ---\")\n",
    "\n",
    "# Task 1: Load data from Phase 1 output\n",
    "df_loaded_p2 = load_data(OUTPUT_P1)\n",
    "print(f\"1. Loaded data for Phase 2. Shape: {df_loaded_p2.shape}\")\n",
    "\n",
    "# Task 2: Impute missing values using the median strategy\n",
    "# Note: 'median' is generally robust to outliers, which is good before outlier treatment.\n",
    "df_imputed = impute_missing_values(df_loaded_p2, strategy='median')\n",
    "print(\"2. Missing values imputed using the median strategy.\")\n",
    "print(f\"   Check for remaining NaNs: {df_imputed.isnull().sum().sum()}\")\n",
    "\n",
    "# Task 3: Treat Outliers using the IQR Capping method\n",
    "df_p2 = treat_outliers_iqr(df_imputed, OUTLIER_COLS, factor=1.5)\n",
    "print(\"3. Outliers treated using IQR capping in specified columns.\")\n",
    "\n",
    "# Task 4: Visualize data after outlier treatment (Box Plots)\n",
    "print(\"\\n4. Box Plots of key features after Outlier Treatment:\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i, col in enumerate(OUTLIER_COLS[:6]): # Display first 6 treated features\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    sns.boxplot(y=df_p2[col])\n",
    "    plt.title(f'Box Plot of {col} (Post-Cleaning)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the processed DataFrame for the next phase\n",
    "df_p2.to_csv(OUTPUT_P2, index=False)\n",
    "print(f\"5. DataFrame state saved to: {OUTPUT_P2}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 5. PHASE 3: DATA TRANSFORMATION (ENGINEERING, ENCODING, SCALING) ---\n",
    "print(\"--- PHASE 3: Data Transformation ---\")\n",
    "\n",
    "# Task 1: Load data from Phase 2 output\n",
    "df_loaded_p3 = load_data(OUTPUT_P2)\n",
    "print(f\"1. Loaded data for Phase 3. Shape: {df_loaded_p3.shape}\")\n",
    "\n",
    "# Task 2: Feature Engineering (Create Age_Group)\n",
    "df_engineered = feature_engineering(df_loaded_p3)\n",
    "print(\"2. Feature 'Age_Group' engineered successfully.\")\n",
    "print(\"   Age_Group distribution:\\n\", df_engineered['Age_Group'].value_counts())\n",
    "\n",
    "# Task 3: Encode the new ordinal feature ('Age_Group')\n",
    "df_encoded = encode_features(df_engineered, 'Age_Group', AGE_GROUP_LABELS)\n",
    "print(\"3. 'Age_Group' encoded and original column dropped.\")\n",
    "print(f\"   New columns: {df_encoded.columns.tolist()}\")\n",
    "\n",
    "# Task 4: Scale numerical data\n",
    "# StandardScaler is chosen as it's less affected by the prior outlier treatment (capping)\n",
    "df_p3, scaler_model = scale_data(df_encoded, scaler_type='StandardScaler')\n",
    "print(\"4. Data scaled using StandardScaler.\")\n",
    "print(f\"   Scaled DataFrame mean (should be ~0): {df_p3.drop(columns=['Class']).mean().mean():.4f}\")\n",
    "print(f\"   Scaled DataFrame std (should be ~1): {df_p3.drop(columns=['Class']).std().mean():.4f}\")\n",
    "\n",
    "# Save the processed DataFrame for the next phase\n",
    "df_p3.to_csv(OUTPUT_P3, index=False)\n",
    "print(f\"5. DataFrame state saved to: {OUTPUT_P3}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 6. PHASE 4: DATA REDUCTION (FEATURE SELECTION/PCA) ---\n",
    "print(\"--- PHASE 4: Data Reduction ---\")\n",
    "\n",
    "# Task 1: Load data from Phase 3 output\n",
    "df_loaded_p4 = load_data(OUTPUT_P3)\n",
    "print(f\"1. Loaded data for Phase 4. Shape: {df_loaded_p4.shape}\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_loaded_p4.drop('Class', axis=1)\n",
    "y = df_loaded_p4['Class']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}, Target (y) shape: {y.shape}\")\n",
    "\n",
    "# Task 2: Feature Selection using SelectKBest\n",
    "X_fs = feature_selection_kbest(X, y, k=K_BEST_FEATURES)\n",
    "selected_features = X_fs.columns.tolist()\n",
    "print(f\"2. SelectKBest selected {K_BEST_FEATURES} features:\")\n",
    "print(f\"   Selected Features: {selected_features}\")\n",
    "\n",
    "# Task 3: Dimensionality Reduction using PCA (Demonstration only, not used for final save)\n",
    "df_pca, pca_model = perform_pca(X, variance_threshold=0.90)\n",
    "explained_variance = np.sum(pca_model.explained_variance_ratio_)\n",
    "print(\"\\n3. PCA Analysis (for context):\")\n",
    "print(f\"   PCA reduced features to {df_pca.shape[1]} components.\")\n",
    "print(f\"   Cumulative Explained Variance: {explained_variance*100:.2f}%\")\n",
    "\n",
    "# Create final reduced dataset (using SelectKBest features for interpretability)\n",
    "df_p4 = pd.concat([X_fs, y], axis=1)\n",
    "print(f\"\\n4. Final reduced dataset shape (SelectKBest): {df_p4.shape}\")\n",
    "\n",
    "# Save the processed DataFrame for the next phase\n",
    "df_p4.to_csv(OUTPUT_P4, index=False)\n",
    "print(f\"5. DataFrame state saved to: {OUTPUT_P4}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 7. PHASE 5: DATA IMBALANCE HANDLING ---\n",
    "print(\"--- PHASE 5: Data Imbalance Handling ---\")\n",
    "\n",
    "# Task 1: Load data from Phase 4 output\n",
    "df_loaded_p5 = load_data(OUTPUT_P4)\n",
    "print(f\"1. Loaded data for Phase 5. Shape: {df_loaded_p5.shape}\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df_loaded_p5.drop('Class', axis=1)\n",
    "y = df_loaded_p5['Class']\n",
    "\n",
    "# Initial Class Distribution\n",
    "print(\"2. Initial Class Distribution:\")\n",
    "initial_counts = Counter(y)\n",
    "print(initial_counts)\n",
    "\n",
    "# Task 2: Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "X_resampled, y_resampled = handle_imbalance_smote(X, y, random_state=42)\n",
    "print(\"\\n3. SMOTE applied successfully.\")\n",
    "\n",
    "# Final Class Distribution\n",
    "print(\"4. Final Class Distribution after SMOTE:\")\n",
    "final_counts = Counter(y_resampled)\n",
    "print(final_counts)\n",
    "\n",
    "# Task 3: Visualize final class distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=y_resampled, palette='viridis')\n",
    "plt.title('Target Class Distribution After SMOTE')\n",
    "plt.xlabel('Class (0: Non-Diabetic, 1: Diabetic)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Combine resampled features and target into the final dataset\n",
    "df_final = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Class')], axis=1)\n",
    "print(f\"\\n5. Final Balanced Dataset Shape: {df_final.shape}\")\n",
    "print(f\"   Final Columns: {df_final.columns.tolist()}\")\n",
    "print(\"   Final DataFrame Head:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Save the FINAL processed DataFrame\n",
    "df_final.to_csv(FINAL_OUTPUT_FILE, index=False)\n",
    "print(f\"\\n*** COMPLETE: FINAL PROCESSED DATASET SAVED TO: {FINAL_OUTPUT_FILE} ***\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
